

\input{../Latex_Templates/Preamble_Report}

%%%%% TITLE PAGE

%\subject{, VT23}
\title{ Project Report for Seminar Course in Numerical Analysis, VT23 \\[1ex]
	  \large Junzi Zhang, Brendan O'Donoghue, Stephen Boyd: Globally Convergent Type-I Anderson Acceleration for Non-Smooth Fixed-Point Iterations}
%\subtitle{}
\author{Theo Koppenh√∂fer}
\date{Lund \\[1ex] \today}

\addbibresource{bibliography.bib}

%%%%% The content starts here %%%%%%%%%%%%%


\begin{document}

\maketitle

\section{Introduction}

The following report will summarise and present the main results of  \cite{ZhaAA} as part of the course NUMN27, Seminar in Numerical Analysis.
First we will motivate the Anderson-acceleration-I (AA-I) algorithm.  Then we discuss the modifications made to the algorithm in \cite{ZhaAA}. After that we will give the main convergence result and finally test the AA-I algorithm in numerical experiments. The report, the python implementation and the corresponding presentation of the topic can be found online under \cite{Repository}.


\begin{figure}
\centering
\begin{algorithm}[H]
\caption{Fixed point iteration (original)}
\label{alg:original}
\SetKwInOut{Input}{Input}

\Input{Initial value $x_0\in\R^n$ and function $f\colon\R^n\to\R^n$.}
\BlankLine
\For{$k=0,1,\dots$}{
	Set $x_{k+1} =f\brk*{x_k}$.
}
\end{algorithm}
\end{figure}

\begin{figure}[b]
\centering
\begin{minipage}{0.41\textwidth}
\begin{algorithm}[H]
\caption{General AA}
\label{alg:aai}
\SetKwInOut{Input}{Input}


\Input{$x_0\in\R^n$ and $f\colon\R^n\to\R^n$.}
\BlankLine
\For{$k=0,1,\dots$}{
	{\black
	Set $f_k =f\brk*{x_k}$.
	
	Choose $\alpha = \alpha^k\in \R^{k+1}$ such that $\sum_i\alpha_i=1$.
  
	Set $x_{k+1} = \sum_i \alpha_if_{i}$.
	}
}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}{0.58\textwidth}
\begin{algorithm}[H]
\caption{AA-II}
\label{alg:aa2}
\SetKwInOut{Input}{Input}

\Input{$x_0\in\R^n$ and $f\colon\R^n\to\R^n$.}
\BlankLine
\For{$k=0,1,\dots$}{
  Set $f_k =f\brk*{x_k}$.
  
  {\black Set $g_k = x_k-f_k$.}
  
  Choose $\alpha\in \R^{k+1}$ such that $\sum_i\alpha_i=1$ {\black and such that $\alpha$ minimises $\norm{\sum_i\alpha_ig_i}$}.
  
 Set $x_{k+1} = \sum_i \alpha_if_{i}$.
}
\end{algorithm}
\end{minipage}
\end{figure}


\section{Motivation of the Anderson-Acceleration algorithm}

In science it is a common problem to find a fixed point $x=f(x)\in\R^n$ of a function $f\colon\R^n\to\R^n$. An equivalent formulation is to find a zero $x\in\R^n$ of the function $g=\Id-f$. In the following we assume that $f$ indeed has a fixed point and that $f$ is non-expansive, i.e.\ we have for all $x,y\in\R^n$ that
\begin{align*}
	\norm{f(x)-f(y)}\leq\norm{x-y}\,.
\end{align*}
We however also assume that $\nabla f$ is unknown which means we cannot use Newton iteration to solve our problem. We assume that our problem is noisy so that we cannot take finite difference derivatives. If the cost of evaluating $f$ is very high then line search becomes unfeasible and if $n$ is very large we are forced to work matrix free. We know that this type of problem can nonetheless be solved by the fixed point iteration described in algorithm \ref{alg:original}.

It can be shown that the fixed point iteration will in this case converge to a fixed point of $f$. This convergence can be slow in practice if the Lipschitz-constant of $f$ is close to $1$. It is here were the Anderson-Acceleration (AA) methods come in.

The main idea of AA methods is to use the information gained from previous function evaluations of $f$ to determine the point $x_{k+1}$. To update $x_{k+1}$ we now form a weighted average as described in algorithm \ref{alg:aai}. For simplicity of notation we assume in the following that our memory is unlimited.

The General AA algorithm requires a specific choice of $\alpha\in\R^{k+1}$. Since finding a fixed point of $f$ is equivalent to finding a zero of $f=\Id-f$ it seems sensible to require $\alpha$ to minimise
\begin{align*}
	\norm3{\sum_i\alpha_ig_i}\,.
\end{align*}
This choice of $\alpha$ yields the AA-II algorithm given in \ref{alg:aa2}.

\begin{figure}
\centering
%	\IncMargin{1em}
\begin{algorithm}[H]
\caption{AA-II (reformulated)}
\label{alg:aa2-ref}
\SetKwInOut{Input}{Input}

\Input{$x_0\in\R^n$ and $f\colon\R^n\to\R^n$.}
\BlankLine
{\black Set $x_1 =f(x_0)$.}

\For{$k=0,1,\dots$}{
	Set $g_k= g(x_k)$.
	
	{\black Construct $S_k=\vect{x_1-x_0 &\cdots& x_{k}-x_{k-1}}\in\R^{n\times k}$ and $Y_k=\vect{g_1-g_0 &\cdots& g_k-g_{k-1}}\in\R^{n\times k}$.
	
	Set $H_k = \Id +(S_k-Y_k)\brk*{Y_k^\top Y_k}^{-1}Y_k^\top\in\R^{n\times n}$.
%		$s_{k-1}= x_k-x_{k-1}$ and
%		$y_{k-1}= g_k-g_{k-1}$.
	
	Set $x_{k+1}= x_k-H_kg_k$.}
}
\end{algorithm}
\end{figure}

It is shown in \cite[Section 2.2]{ZhaAA} that this update can be brought into the form of a quasi-Newton-like method given in algorithm \ref{alg:aa2-ref}. This method bears a lot of similarities with the bad Broyden method where $H_k$ is an approximate inverse of $\nabla f(x_k)$. Indeed one can show
\begin{proposition}[Approximate inverse Jacobian]
	In algorithm \ref{alg:aa2-ref} the matrix $H_k$ minimises $\norm{H_k-\Id}_F$ under the multi-secant condition $H_kS_k=Y_k$.
\end{proposition}
\begin{proof}
	See \cite[Section 2.2]{ZhaAA}.
\end{proof}
The good Broyden method approximates the Jacobian rather than its inverse and tends to yield better results than the bad Broyden. This motivates to choose  $B_k=H_k^{-1}$ to be a minimiser of $\norm{B_k-\Id}_F$ under the condition $B_kY_k=S_k$.
If one chooses $B_k$ in this way it is motivated in \cite[Section 2.3]{ZhaAA} that
\begin{align*}
	B_k = \Id+\brk*{Y_k-S_k}\brk*{S_k^\top S_k}^{-1}S_k^\top\,.
\end{align*}
This yields the AA-I algorithm \ref{alg:aa1}.

\begin{figure}
\centering
\begin{algorithm}[H]
\caption{AA-I}
\label{alg:aa1}
\SetKwInOut{Input}{Input}

\Input{$x_0\in\R^n$ and $f\colon\R^n\to\R^n$.}
\BlankLine
Set $x_1=f(x_0)$

\For{$k=0,1,\dots$}{
	Set $g_k= g(x_k)$.
	
	Construct $S_k$ from $x_0,\dots, x_k$ and $Y_k$ from $g_0,\dots,g_k$.
			
	{\black Set $B_k = \Id+\brk*{Y_k-S_k}\brk*{S_k^\top S_k}^{-1}S_k^\top\in\R^{n\times n}$.}
	
	Set $x_{k+1}= x_k-H_kg_k$ with $H_k=B_k^{-1}$.
}
\end{algorithm}
\end{figure}

\newpage
\section{Modifications of the AA-I algorithm}

The AA-I algorithm as stated in \ref{alg:aa1} has some issues. For one, the approach is not matrix-free. This is fixed by a rank-1 update formula for matrices $B_k$ and later for $H_k$. It may also occur in a step that the matrix $H_k$ is not well-defined. This may occur if $B_k$ itself is not well-defined or is singular. The well-definedness will be resolved by the Powell-type regularisation and the restarting of the iteration. The restarting of the iteration will also yield an algorithm that does not require unlimited memory. Lastly, this algorithm does not have the convergence property of the fixed point iteration algorithm. This will be resolved by adding a safeguarding of steps.
	
	
We start with the rank-1 update formula for $B_k$. One can show that
\begin{proposition}[Rank-1 update for $B_k$]
	We have
	\begin{align}
		B_{k} = B_{k-1}+\frac{\brk*{y_{k-1}-B_{k-1}s_{k-1}}\hs_{k-1}^\top}{\hs_{k-1}^\top s_{k-1}}\label{eq:Bk_rank1}
	\end{align}
	where $y_{k-1} = g_{k}-g_{k-1}$, $B_0=\Id$ and
	\begin{align*}
		\hs_{k-1} = s_{k-1}-\sum_{j=0}^{k-2}\frac{\hs_j^\top s_{k-1}}{\norm{\hs_j}^2}\hs_j
	\end{align*}
	is the Gram-Schmidt orthogonalisation of $s_{k-1}=x_{k}-x_{k-1}$.
\end{proposition}
\begin{proof}
	See \cite{ZhaAA}.
\end{proof}
To fix the potential (approximate) singularity of $B_k$ we use Powell-type regularisation. This means that instead of using equation \ref{eq:Bk_rank1} to update $B_k$ one uses the following modification
\begin{align*}
		B_{k} = B_{k-1}+\frac{\brk*{\tiy_{k-1}-B_{k-1}s_{k-1}}\hs_{k-1}^\top}{\hs_{k-1}^\top s_{k-1}}\,.
\end{align*}
Here $\tiy_{k-1}=\theta_{k-1}y_{k-1}-(1-\theta_{k-1})g_{k-1}$ where $\theta_{k-1}\in\R$ is chosen in dependence of a parameter $\bartheta$.
This is a modification of the update of $B_k$ is given in algorithm \ref{alg:aa1-pr}.

If $\hs_k\approx 0$ the update of $B_k$ in ($*$) in algorithm \ref{alg:aa1-pr} becomes unstable and for $\hs_k=0$ ill-defined.
Hence we restart the algorithm with $x_k$ as the new starting point if
\begin{itemize}
	\item $k=m+1$ for some fixed $m\in\N$ or
	\item $\norm{\hs_{k-1}}<\tau\norm{s_{k-1}}$ for some fixed $\tau\in(0,1)$.
\end{itemize}
It can be shown that $B_k$ is then well-defined.

\begin{figure}[h]
\centering
\begin{algorithm}[H]
\caption{AA-I with Powell-type regularisation and Restarting}\label{alg:aa1-pr}
\SetKwInOut{Input}{Input}

\Input{$x_0\in\R^n$, $f\colon\R^n\to\R^n$, {\black$m \in\N$ }and $\bartheta{, \black\tau}\in(0,1)$}
\BlankLine
Set $B_0=\Id$, $x_1=f\brk{x_0}$ and {\black $m_0 = 0$}.

\For{$k=0,1,\dots$}{
	Set $g_k= g(x_{k})$,
	{\black $m_k= m_{k-1}+1$}, 
	$s_{k-1}= x_k-x_{k-1}$ and
	$y_{k-1}= g_k-g_{k-1}$.
	
	Set $\hs_{k-1}= s_{k-1}-\sum_{i=k-m_k}^{k-2}\frac{\hs_i^\top s_{k-1}}{\norm{\hs_i}^2}s_i$.
	
	{\black
	\If{$m_k=m+1$ or $\norm{\hs_{k-1}}<\tau\norm{s_{k-1}}$}{
		Set $m_k=0$, $\hs_{k-1}= s_{k-1}$ and $B_{k-1}=\Id$.
	}
	}
	Choose $\theta_{k-1}$ in dependence of $\bartheta$.
	
	Set $\tiy_{k-1}=\theta_{k-1}y_{k-1}-(1-\theta_{k-1})g_{k-1}$.
	
	Set $B_k = B_{k-1}+\frac{\brk*{\tiy_{k-1}-B_{k-1}s_{k-1}}\hs_{k-1}^\top}{\hs_{k-1}^\top s_{k-1}}$.\brkcomment*[r]{*}
	
	Set $x_{k+1}= x_k-H_kg_k$ with $H_k=B_k^{-1}$.
	
}
\end{algorithm}
\end{figure}

One can then show
\newconstant{upperHk}
\begin{lemma}[bound on $\norm{H_k}_2$]\label{le:H_kUpperBound}
	In algorithm \ref{alg:aa1-pr} we have that $H_k$ is well-defined and there exists a constant $\useconstant{upperHk}=\useconstant{upperHk}(m,n, \bartheta,\tau)>0$ such that
	\begin{align*}
		\norm{H_k}_2\leq \useconstant{upperHk}\,.
	\end{align*}
\end{lemma}
\begin{proof}
	See \cite[Corollary 4]{ZhaAA} for details. An outline of the proof is given in figure \ref{fig:Diagram_001}. The proof uses two Lemmas which provide a bound of $\norm{B_k}$ from above and a bound of $\abs{\det B_k}$ from below.
	
	\begin{figure}
	\centering
%	\tikzexternaldisable
	\scalebox{0.7}{{\normalsize
	\input{../Figures/Diagram_001}
	}}
	\caption{Outline of proof of lemma \ref{le:H_kUpperBound}.}
	\label{fig:Diagram_001}
%	\tikzexternalenable
	\end{figure}
\end{proof}
This bound on $\norm{H_k}_2$ will later be used when showing convergence of the final algorithm. One can now replace the update formula for the $B_k$ directly with an update formula for $H_k$ by the following result.
\begin{proposition}[Rank-1 update for $H_k$]
	We have
	\begin{align*}
		H_{k} = H_{k-1}+\frac{\brk*{s_{k-1}-H_{k-1}y_{k-1}}\hs_{k-1}^\top H_{k-1}}{\hs_{k-1}^\top H_{k-1}y_{k-1}}
	\end{align*}
\end{proposition}
This formula has been incorporated in algorithm \ref{alg:aa1-prs}.
Note that the convergence behaviour of the algorithm \ref{alg:aa1-pr} is not optimal.
To guarantee the decrease in $\norm{g_k}$ one can interleave the AA-I steps with Krasnosel'skii-Mann (KM) steps which are given by
\begin{align*}
	x_{k+1}= (1-\alpha)x_k +\alpha f_k
\end{align*}
for some fixed $\alpha\in(0,1)$. This yields the final algorithm \ref{alg:aa1-prs}.

\begin{figure}[h]
\centering
\begin{algorithm}[H]
\caption{AA-I with Powell-type regularisation, restarting and safeguarding}\label{alg:aa1-prs}
\SetKwInOut{Input}{Input}

\Input{$x_0\in\R^n$, $f\colon\R^n\to\R^n$,$m \in\N$, $\bartheta, \tau, {\black\alpha}\in(0,1)$ and {\black safe-guarding constants $D,\e>0$}}
\BlankLine
Set $H_0=\Id$, $x_1={\black\tix_1=f\brk{x_0}}$, $m_0 = {\black n_{AA}=0}$ and ${\black\barU=\norm{g_0}}$.

\For{$k=0,1,\dots$}{
	Set $g_k= g(x_{k})$,
	$m_k= m_{k-1}+1$, 
	$s_{k-1}= {\black\tix_k}-x_{k-1}$ and
	$y_{k-1}= g({\black\tix_k})-g_{k-1}$.
	
	Set $\hs_{k-1}= s_{k-1}-\sum_{i=k-m_k}^{k-2}\frac{\hs_i^\top s_{k-1}}{\norm{\hs_i}^2}s_i$.
	
	\If{$m_k=m+1$ or $\norm{\hs_{k-1}}<\tau\norm{s_{k-1}}$}{
		Set $m_k=0$, $\hs_{k-1}= s_{k-1}$ and $H_{k-1}=\Id$.
	}
	Choose $\theta_{k-1}$ in dependence of $\bartheta$.
	
	Set $\tiy_{k-1}=\theta_{k-1}y_{k-1}-(1-\theta_{k-1})g_{k-1}$.
	
	Set $H_k = H_{k-1}+\frac{(s_{k-1}-H_{k-1}\tiy_{k-1})\hs_{k-1}^\top H_{k-1}}{\hs_{k-1}^\top H_{k-1}\tiy_{k-1}}$ and $\tix_{k+1}= x_k-H_kg_k$.
	
	{\black
	\uIf{$\norm{g_k}\leq D\barU(n_{AA}+1)^{-(1+\e)}$}{
		Set $x_{k+1}=\tix_{k+1}$ and $n_{AA}= n_{AA}+1$.
	}
	\Else{
		Set $x_{k+1}= (1-\alpha)x_k +\alpha f_k$.
	}
	}
}
\end{algorithm}
\end{figure}


\newpage

\clearpage

\section{Convergence result}

In this section we state and prove the main convergence result in three parts.

\begin{theorem}[Convergence]
	Let $x_k$ be generated by algorithm \ref{alg:aa1-prs} then $x_k$ converges to a fixed point of $f$.
\end{theorem}

\begin{proof}
The proof follows \cite[Theorem 6]{ZhaAA}. In the first part we use lemma \ref{le:H_kUpperBound} on the boundedness of $\norm{H_k}_2$ and the safe-guarding step to show the convergence of $g_k$ to $0$. In part 2 we also use lemma \ref{le:H_kUpperBound} and the safe-guarding to show convergence of $a_k=\norm{x_k-y}^2$ to some $a\in\R$. Here $y$ denotes a fixed point of $f$. In the third part we use parts 1 and 2 to show that $x_k$ converges to a fixed point.

\textit{Part 1.}
We partition $\N=K_{AA}\sqcup K_{KM}$ where $K_{AA}=\brk[c]{k_0,k_1,\dots}$ denote the indices $k$ where the algorithm chose an AA-step (a) and $K_{KM}=\brk[c]{l_0,l_1,\dots}$ where the algorithm chose a KM-step (b).

\begin{center}
\begin{algorithm}[H]
	\uIf{$\norm{g_k}\leq D\barU(n_{AA}+1)^{-(1+\e)}$}{
		Set $x_{k+1}=\tix_{k+1}$ and $n_{AA}= n_{AA}+1$. \brkcomment*[r]{a}
	}
	\Else{
		Set $x_{k+1}= (1-\alpha)x_k +\alpha f_k$. \brkcomment*[r]{b}
	}
\caption{The two cases for $x_{k+1}$.}
\end{algorithm}
\end{center}

Let $y$ be a fixed point of $f$. We distinguish the cases
\begin{description}
	\item[case (a)]
	if $k_i\in K_{AA}$ then
	\newconstant{CDU}
	\begin{equation}
	\begin{aligned}
		\norm{x_{k_i+1}-y}&\leq \norm{x_{k_i}-y}+\norm{H_{k_i}g_{k_i}} \\
		&\leq \norm{x_{k_i}-y}+\useconstant{upperHk}\norm{g_k} \\
		&\leq \norm{x_{k_i}-y}+\useconstant{CDU}(i+1)^{-(1+\e)}
		\label{eq:16}
	\end{aligned}
	\end{equation}
	for some constant $\useconstant{CDU}>0$.
	\item[case (b)]
	if $l_i\in K_{KM}$ then one can show (see \cite[Theorem 6]{ZhaAA})
	\begin{align}
		\norm{x_{l_i+1}-y}^2\leq \norm{x_{l_i}-y}^2-\alpha(1-\alpha)\norm{g_{l_i}}^2
		\label{eq:17}\,.
	\end{align}
	Here one uses the non-expansiveness of $f$ and that $y$ is a fixed point.
\end{description}
Hence we have in any case
\newconstant{E}
\begin{align*}
	\norm{x_k-y}
	\leq \norm{x_0-y}+\useconstant{CDU}\sum_i(i+1)^{-(1+\e)}
	= \useconstant{E}<\infty\,.
\end{align*}
It then follows that
\begin{equation}
\begin{aligned}
	a_{{k_i}+1} &= \norm{x_{{k_i}+1}-y}^2\stackrel{\eqref{eq:16}, \eqref{eq:17}}{\leq}\brk*{\norm{x_{k_i}-y}+\useconstant{CDU}(i+1)^{-(1+\e)}}^2 \\
	&\leq \underbrace{\norm{x_{k_i}-y}^2}_{=a_{k_i}}+\underbrace{\useconstant{CDU}^2(i+1)^{-2(1+\e)}+2\useconstant{CDU}\overbrace{\norm{x_{k_i}-y}}^{\leq \useconstant{E}}(i+1)^{-(1+\e)}}_{=b_{k_i}} \\
	&= a_{k_i}+b_{k_i}
\end{aligned}
\label{eq:19}
\end{equation}
and thus
\begin{align*}
	\alpha(1-\alpha)\sum_i\norm{g_{l_i}}^2
	\stackrel{\eqref{eq:17}}{\leq} \sum_i a_{l_i}-a_{l_i+1}
	\stackrel{\eqref{eq:19}}{\leq}a_0+\sum_k b_k
	<\infty\,.
\end{align*}
We therefore have $\lim_i\norm{g_{l_i}}=0$. It also follows from $\norm{g_{k_i}}\leq D\barU (i+1)^{-(1+\e)}$ that $\lim_i\norm{g_{k_i}}=0$. As all subsequences of $g_k$ converge to $0$ we thus have that $g_k$ converges to $0$.

\textit{Part 2.}
Let now $a_{k_i}$ and $a_{l_i}$ be subsequences such that $k_i\leq l_i$ and
\begin{align*}
	a_{k_i}\xrightarrow{j\to\infty}\liminf_ka_k=\underline{a} \quad \text{and} \quad
	a_{l_i}\xrightarrow{j\to\infty}\limsup_ka_k=\overline{a}\,.	
\end{align*}
It then follows that
\begin{align*}
	a_{l_i}-a_{k_i}
	= \sum_{k=k_i}^{l_i-1}a_{k+1}-a_k
	\stackrel{\eqref{eq:19}}{\leq} \sum_{k=k_i}^\infty b_k
\end{align*}
and when taking $l_i\to\infty$
\begin{align*}
	\overline{a}-a_{k_i}\leq \sum_{k=k_i}^\infty b_k
\end{align*}
and then taking $k_i\to\infty$ we get
\begin{align*}
	\overline{a}-\underline{a}\leq 0\,.
\end{align*}
Thus
\begin{align*} 
	\limsup_ka_k=\overline{a}\leq \underline{a}=\liminf_ka_k
\end{align*}
and so $a_k=\norm{x_k-y}^2$ converges to some $a\in\R$.

\textit{Part 3.}
Let $k_i$ and $l_i$ now be convergent subsequences of $x_k$ which converge to $x$ and $\tix$ respectively. Since by continuity of $g$
\begin{align*}
	\norm{g(x)}=\lim_i\norm{g(x_{k_i})}\stackrel{\text{part 1}}{=}0
\end{align*}
we have that $x$ is a fixed point and analogously $\tix$ is too.
Now we have
\begin{align*}
	\norm{x_{k_i}}^2
	= \norm{x_{k_i}-y}^2-\norm{y}^2+2y^\top x_{k_i}
\end{align*}
and by part 2 we obtain when taking $i\to\infty$
\begin{align*}
	\norm{x}^2
	= a-\norm{y}^2+2y^\top x
\end{align*}
Analogously we obtain
\begin{align*}
	\norm{\tix}^2=a-\norm{y}^2+2y^\top \tix
\end{align*}
which implies
\begin{align*}
	2y^\top(x-\tix) = \norm{x}^2-\norm{\tix}^2\,.
\end{align*}
As $x$ and $\tix$ are fixed points it follows for $y\in\brk[c]{x,\tix}$ that
\begin{align*}
	x^\top(x-\tix) = \tix^\top(x- \tix)
\end{align*}
and further
\begin{align*}
	(x-\tix)^\top(x-\tix) = 0\,.
\end{align*}
We thus have $x=\tix$. We have shown that two convergent subsequences of $x_k$ have the same limit and hence $x_k$ is convergent and the limit must be a fixed point of $f$.

\end{proof}


\section{Numerical experiments}

As part of the project some numerical experiments from \cite{ZhaAA} were replicated. The obtained results are largely the same as in \cite{ZhaAA}. The aim of the experiments is to test the numerical performance of the algorithms. The functions $f$ are chosen with Lipschitz constant close to $1$ as these are precisely the type of problem for which the AA algorithm was developed.

\subsection{Elastic net regression}

In the first experiment $f$ originates from an elastic net regression problem and is motivated in \cite[Section 5.1f]{ZhaAA}. Specifically one obtains
\begin{align*}
	f\colon\R^{n}\to\R^{n}, \qquad x\mapsto S_{\alpha\mu/2}\brk*{x-\alpha\brk*{A^\top(Ax-b)+\frac{\mu}{2}x}}
\end{align*}
with shrinkage operator
\begin{align*}
	S_\kappa(x) = \brk*{\sgn(x_i)\brk*{\abs{x_i}-\kappa}_+}_{i=1}^{n}
\end{align*}
and $A\in\R^{m\times n}$, $b\in\R^{m}$ and some $\alpha, \mu\in\R$. Here we choose the parameters as in \cite[Section 5.2]{ZhaAA}. In particular we choose $m=500$ and $n=1000$ and $A$, $b$ and $x_0$ to be randomly generated.

\begin{figure}
	\centering
	{\scriptsize
	\input{../Plots/method_comparison_ISTA.pgf}
	}
	\caption{Residual norms for the elastic net regression problem.}
	\label{pl:method_comparison_ISTA}
\end{figure}

The results for the different methods can be seen in Figure \ref{pl:method_comparison_ISTA}. Here the method 'original' refers to the fixed point iteration, i.e.\ algorithm \ref{alg:original}. The method 'aa1-safe' is the AA-I algorithm with Powell-type regularisation, restarting and safeguarding. The 'aa1-matrix' and 'aa2-matrix' algorithms are an implementation of the AA-I and AA-II algorithms given in \ref{alg:aa1} and \ref{alg:aa2} with limited memory. Here 'matrix' indicates that the implementation is not matrix-free. We see that the full-matrix implementations do not outperform the 'original' method and behave quite similarly. The 'aa1' and 'aa1-safe' methods behave quite similarly. Both methods however outperform the other methods on this problem.

%\begin{figure}
%	\centering
%	{\scriptsize
%	% \tiny
%	\input{../Plots/memory_comparison_ISTA.pgf}
%	}
%	\caption{Residual norms for the elastic net regression problem.}
%	\label{pl:memory_comparison_ISTA}
%\end{figure}
%
%In figure \ref{pl:memory_comparison_ISTA} we see how the performance of the 'aa1-safe' method depends on the memory parameter 'm'.

\subsection{Markov decision process}

In a second experiment $f$ originates from a random Markov decision process which is motivated in \cite[Section 5.1f]{ZhaAA}. Our aim is to find a fixed point of the Bellman operator
\begin{align*}
	f\colon\R^{n}\to\R^{n}, \qquad x\mapsto \brk*{\max_{a} \brk3{ R_{sa}+\gamma \sum_{s'}P_{sas'}x_{s'}}}_{s=1}^{n}
\end{align*}
with some $R\in\R^{S\times A}$, $P\in\R^{S\times A\times A}$ and $\gamma\in\R$. Here the parameter $\gamma$ determines the Lipschitz-constant of $f$. Again, we choose the parameters as in \cite[Section 5.2]{ZhaAA}. In particular we choose $n=1000$, $A=200$, $S=300$ and $A$ and $R$ to be randomly generated.

\begin{figure}
	\centering
	{\scriptsize
	\input{../Plots/method_comparison_VI.pgf}
	}
	\caption{Residual norms for the Markov decision process problem.}
	\label{pl:method_comparison_VI}
\end{figure}

The performance for the various methods can be seen in figure \ref{pl:method_comparison_VI}. In contrast to the other methods the the 'aa2-matrix' method does not converge here. In this problem the fixed point iteration ('original') converges very slowly. The 'aa1-safe' method outperforms all others. This confirms numerically that the 'aa1-safe' algorithm can deal with the problems it was specifically designed for and for which the fixed point iteration fails to converge in adequate time. 

\begin{figure}
	\centering
	{\scriptsize
	% \tiny
	\input{../Plots/memory_comparison_VI.pgf}
	}
	\caption{Residual norms for the Markov decision process problem.}
	\label{pl:memory_comparison_VI}
\end{figure}

In figure \ref{pl:memory_comparison_VI}  we see how the performance of the 'aa1-safe' method depends on the memory parameter $m$. In particular one sees that the algorithm performs best for this problem with a parameter of $m\approx10$. We also see that increasing the parameter $m$ does not necessarily improve performance of the method as in this plot the choice $m=50$ performs worst.

\newpage

\section{Summary}

The AA-I algorithm is specifically tailored to find the fixed point of a function $f$ which is expensive to evaluate, noisy, has an unknown gradient and where the dimension $n$ is large. The main idea of the AA-I and AA-II algorithms is to generalise the fixed point iteration by setting $x_{k+1}=\sum_i\alpha_if_i$ for some clever choice of $\alpha=\alpha^k\in\R^{k+1}$. The AA-I algorithm one obtains requires some modifications. More specifically, one applies Powell-type regularisation and a restarting of the iteration for well-definedness. One builds in a mechanism for safeguarding of the steps for convergence and one uses a rank-1 update formula to make the implementation matrix-free. One can then show the convergence of the algorithm under the assumption that $f$ is non-expansive and that there exists a fixed point. The numerical experiments then show that the AA-I algorithm with the modifications outperforms the fixed point iteration for the problems tested.



\section*{Bibliography}
\nocite{*}
Main source
\printbibliography[heading=none, keyword={main}]
\noindent Other sources
\printbibliography[heading=none, keyword={secondary}]

\end{document}

%

\input{../Latex_Templates/Preamble_Presentation}

%%%%% TITLE PAGE

\subject{, VT23}
\title{Project presentation of \\[1ex]
{\large Zhang, et al.: Globally Convergent Type-I Anderson Acceleration for Non-Smooth Fixed-Point Iterations}}
%\subtitle{}
\author{Theo KoppenhÃ¶fer}
\date{Lund \\[1ex] \today}


%\SetAlFnt{\small}
\addbibresource{../Presentation/bibliography.bib}



\begin{document}

%\frame[plain]



% Frame 2
\frame[plain]{\titlepage}

% Frame 3
\frame[plain]{ \frametitle{Table of contents} \tableofcontents }

\section{The problem setting}
\begin{frame}
	\frametitle{The problem setting}
	\begin{problem}[find fixed point]
		Find a fixed point $x\in\R^n$ of $f\colon\R^n\to\R^n$, i.e.\ $x=f(x)$.
	\end{problem}
	or equivalently
	\begin{problem}[find zero]
		Find a zero $x\in\R^n$ of $g=\Id-f$, i.e.\ $0=g(x)$.
	\end{problem}
	We also assume
	\begin{itemize}
		\item $f$ has a fixed point.
		\item $f$ is nonexpansive, i.e.\ $\norm{f(x)-f(y)}\leq\norm{x-y}$.
		\item $\nabla f$ is unknown \textrightarrow no Newton
		\item noisy problem \textrightarrow no finite difference derivatives
		\item cost of evaluating $f$ is high \textrightarrow no line search
		\item $n$ is large \textrightarrow matrix-free
	\end{itemize}
\end{frame}

\section{Motivation of AA-I}
\subsection{Fixed point iteration}
\begin{frame}
	\frametitle{Fixed point iteration}
	To keep things simple we try

	\begin{figure}
	\begin{algorithm}[H]
	\caption{Fixed point iteration (original)}
	\SetKwInOut{Input}{Input}
	
	\Input{Initial value $x_0\in\R^n$ and function $f\colon\R^n\to\R^n$.}
	\BlankLine
	\For{$k=0,1,\dots$}{
		Set $x_{k+1} =f\brk*{x_k}$.
	}
	\end{algorithm}
	\end{figure}
\end{frame}


\begin{frame}
	\begin{block}{This works, but ...} \vspace{1cm}
		\includegraphics[scale=0.25]{../Figures/turtle}	
	\end{block}
\end{frame}
\begin{frame}
	\begin{block}{We want to be like...} \vspace{2cm}
		\includegraphics[scale=0.17]{../Figures/hare}
	\end{block}
\end{frame}

\subsection{General AA}
\begin{frame}
	\frametitle{General AA}
	We may as well use the information gained from previous evaluations. In the following we assume for simplicity that our memory is unlimited. If we form a weighted average we get
	
	\begin{figure}
	\begin{algorithm}[H]
	\caption{General AA (Anderson Acceleration)}
	\SetKwInOut{Input}{Input}
	\color{gray}
	
	\Input{$x_0\in\R^n$ and $f\colon\R^n\to\R^n$.}
	\BlankLine
	\For{$k=0,1,\dots$}{
		{\black
		Set $f_k =f\brk*{x_k}$.
		
		Choose $\alpha = \alpha^k\in \R^{k+1}$ such that $\sum_i\alpha_i=1$.
	  
		Set $x_{k+1} = \sum_i \alpha_if_{i}$.
		}
	}
	\end{algorithm}
	\end{figure}
\end{frame}

\subsection{AA-II}
\begin{frame}
	\frametitle{AA-II}
%	\IncMargin{1em}
	Since finding a fixed point of $f$ is equivalent to finding a zero of $g=\Id-f$ the following seems like a good idea
	
	\begin{figure}
	\begin{algorithm}[H]
	\caption{AA-II}
	\SetKwInOut{Input}{Input}
	\color{gray}
	\Input{$x_0\in\R^n$ and $f\colon\R^n\to\R^n$.}
	\BlankLine
	\For{$k=0,1,\dots$}{
	  Set $f_k =f\brk*{x_k}$.
	  
	  {\black Set $g_k = x_k-f_k$.}
	  
	  Choose $\alpha\in \R^{k+1}$ such that $\sum_i\alpha_i=1$ {\black and such that $\alpha$ minimises $\norm{\sum_i\alpha_ig_i}_2$}.
	  
	 Set $x_{k+1} = \sum_i \alpha_if_{i}$.
	}
	\end{algorithm}
	\end{figure}
\end{frame}

%\begin{frame}
%	\frametitle{Rewriting AA-II}
%	Setting 
%	\begin{align*}
%		\alpha=\vect{\gamma_0 \\ \gamma_1-\gamma_0 \\ \vdots \\ \gamma_{k}-\gamma_{k-1} \\ 1-\gamma_k}
%		\text{ and }
%		Y_k = \begin{bmatrix}
%			& & \\
%			g_{1}-g_0 & \cdots & g_{k}-g_{k-1} \\
%			& &
%		\end{bmatrix} \in\R^{n\times k}
%	\end{align*}
%	one obtains the least squares problem
%	\begin{align*}
%		\min_{\substack{\alpha\in\R^{k+1} \\ \sum_i\alpha_i=1}}\norm2{\sum_i\alpha_ig_i}
%		= \min_{\gamma \in\R^k}\norm{g_k-Y_k\gamma}
%	\end{align*}
%	which is solved by
%	\begin{align*}
%		\gamma= \gamma^k = \brk*{Y_k^\top Y_k}^{-1}Y_k^\top g_k\,.
%	\end{align*}
%\end{frame}
%
%\begin{frame}
%	If we now set
%	\begin{align*}
%		S_k = \begin{bmatrix}
%			& & \\
%			x_1-x_0 & \cdots & x_{k}-x_{k-1} \\
%			& & \\
%		\end{bmatrix} \in\R^{n\times k}
%	\end{align*}
%	we see that
%	\begin{align*}
%		S_k -Y_k &= \begin{bmatrix}
%			& & \\
%			x_1-x_0-(g_1-g_0) & \cdots & x_{k}-x_{k-1}-(g_{k}-g_{k-1}) \\
%			& & \\
%		\end{bmatrix} \\
%		&= \begin{bmatrix}
%			& & \\
%			f_1-f_0 & \cdots & f_k-f_{k-1} \\
%			& & \\
%		\end{bmatrix}
%	\end{align*}
%\end{frame}
%
%\begin{frame}
%	and hence
%	\begin{align*}
%		x_{k+1} &= \sum_i\alpha_if_i \\
%		&= f_k -(S_k-Y_k)\gamma \\ \\
%		&\tikzmark{def_gamma}{=} x_k -\underbrace{\brk*{\Id+(S_k-Y_k)\brk*{Y_k^\top Y_k}^{-1}Y_k^\top}}_{=H_k}g_k \\
%		&= x_k -H_kg_k\,.
%	\end{align*}
%	\tikzset{external/export=false}
%	\begin{tikzpicture}[remember picture, overlay, node distance = 0.3cm]
%		\node[,text width=10cm] (def_gamma_descr) [above right=0.2cm and 0.1cm of def_gamma]{$f_k=x_k-g_k$ and $\gamma=\brk*{Y_k^\top Y_k}^{-1}Y_k^\top$};
%		\draw[,->,thick] (def_gamma_descr) to [in=90,out=180] (def_gamma);
%	\end{tikzpicture}%
%\end{frame}

\begin{frame}
	\frametitle{AA-II (reformulated)}
	One can show that this can be brought into the form of a quasi-Newton-like method
	\begin{figure}
%	\IncMargin{1em}
	\begin{algorithm}[H]
	\caption{AA-II (reformulated)}
	\SetKwInOut{Input}{Input}
	\color{gray}
	\Input{$x_0\in\R^n$ and $f\colon\R^n\to\R^n$.}
	\BlankLine
	{\black Set $x_1 =f(x_0)$.}
	
	\For{$k=0,1,\dots$}{
		Set $g_k= g(x_k)$.
		
		{\black Construct $S_k=\vect{x_1-x_0 &\cdots& x_{k}-x_{k-1}}\in\R^{n\times k}$ and $Y_k=\vect{g_1-g_0 &\cdots& g_k-g_{k-1}}\in\R^{n\times k}$.
		
		Set $H_k = \Id +(S_k-Y_k)\brk*{Y_k^\top Y_k}^{-1}Y_k^\top\in\R^{n\times n}$.
%		$s_{k-1}= x_k-x_{k-1}$ and
%		$y_{k-1}= g_k-g_{k-1}$.
		
		Set $x_{k+1}= x_k-H_kg_k$.}
	}
	\end{algorithm}
	\end{figure}
\end{frame}


\subsection{AA-I}
\begin{frame}
	\frametitle{AA-I}
	This is the form of a quasi-Newton-like method so one could expect $H_k$ to be an approximate inverse of $\nabla f(x_k)$. Indeed
	\begin{proposition}[Approximate inverse Jacobian]
		$H_k$ minimises $\norm{H_k-\Id}_F$ under the multisecant condition $H_kS_k=Y_k$.
	\end{proposition}
	From Broydens method we know that it is a good idea to approximate the Jacobian rather than its inverse.
	\begin{definition}[Approximate Jacobian]
		Let $B_k$ be minimiser of $\norm{B_k-\Id}_F$ under the condition $B_kY_k=S_k$.
	\end{definition}
	One can show that
	\begin{align*}
		B_k = \Id+\brk*{Y_k-S_k}\brk*{S_k^\top S_k}^{-1}S_k^\top\,.
	\end{align*}
\end{frame}

\begin{frame}
	This yields the AA-I algorithm
	\begin{figure}
%	\IncMargin{1em}
	\begin{algorithm}[H]
	\caption{AA-I}
	\SetKwInOut{Input}{Input}
	\color{gray}
	\Input{$x_0\in\R^n$ and $f\colon\R^n\to\R^n$.}
	\BlankLine
	Set $x_1=f(x_0)$
	
	\For{$k=0,1,\dots$}{
		Set $g_k= g(x_k)$.
		
		Construct $S_k$ from $x_0,\dots, x_k$ and $Y_k$ from $g_0,\dots,g_k$.
				
		{\black Set $B_k = \Id+\brk*{Y_k-S_k}\brk*{S_k^\top S_k}^{-1}S_k^\top\in\R^{n\times n}$.
		
		Set $H_k = B_k^{-1}$.}
%		$s_{k-1}= x_k-x_{k-1}$ and
%		$y_{k-1}= g_k-g_{k-1}$.
		
		Set $x_{k+1}= x_k-H_kg_k$.
	}
	\end{algorithm}
	\end{figure}
\end{frame}

\section{Modifications to AA-I}
\begin{frame}
	But this algorithm has some problems
	\begin{itemize}
		\item computational efficiency: the approach is not matrix-free \textrightarrow rank-1 update for $B_k$ and later $H_k$
		\item well-definedness of $H_k$: $B_k$ might not be well-defined or singular \textrightarrow Powell-type regularisation, restarting iteration
		\item memory usage: though infinite memory is nice to have it is not very realistic \textrightarrow restarting iteration
		\item convergence: the algorithm does not necessarily converge \textrightarrow safeguarding steps
	\end{itemize}
\end{frame}

\subsection{Computational efficiency: Rank-1 update for $B_k$}

\begin{frame}
	\frametitle{Computational efficiency: Rank-1 update for $B_k$}
	One can show
	\begin{proposition}[Rank-1 update for $B_k$]
		We have
		\begin{align*}
			B_{k} = B_{k-1}+\frac{\brk*{y_{k-1}-B_{k-1}s_{k-1}}\hs_{k-1}^\top}{\hs_{k-1}^\top s_{k-1}}
		\end{align*}
		where $y_{k-1} = g_{k}-g_{k-1}$, $B_0=\Id$ and
		\begin{align*}
			\hs_{k-1} = s_{k-1}-\sum_{j=0}^{k-2}\frac{\hs_j^\top s_{k-1}}{\norm{\hs_j}^2}\hs_j
		\end{align*}
		is the Gram-Schmidt orthogonalisation of $s_{k-1}=x_{k}-x_{k-1}$.
	\end{proposition}
	\begin{proof}
		See \cite{ZhaAA}.
	\end{proof}
\end{frame}

\begin{frame}
	\begin{figure}
%	\IncMargin{1em}
	\begin{algorithm}[H]
	\caption{AA-I (rank-1 update)}
	\SetKwInOut{Input}{Input}
	\color{gray}
	\Input{$x_0\in\R^n$ and $f\colon\R^n\to\R^n$.}
	\BlankLine
	Set {\black $B_0=\Id$} and $x_1=f\brk{x_0}$.
	
	\For{$k=0,1,\dots$}{
		Set $g_k= g(x_{k})$.
		
		{\black
		Set $s_{k-1}= x_k-x_{k-1}$,
		$y_{k-1}= g_k-g_{k-1}$ and
		$\hs_{k-1}= s_{k-1}-\sum_{i=0}^{k-2}\frac{\hs_i^\top s_{k-1}}{\norm{\hs_i}^2}s_i$.
		
		Set $B_k = B_{k-1}+\frac{\brk*{y_{k-1}-B_{k-1}s_{k-1}}\hs_{k-1}^\top}{\hs_{k-1}^\top s_{k-1}}$.}
		
		Set $H_k=B_k^{-1}$.
		
		Set $x_{k+1}= x_k-H_kg_k$.
		
	}
	\end{algorithm}
	\end{figure}
\end{frame}


\subsection{Well-definedness of $H_k$: Powell-type regularisation}

\begin{frame}	
	\frametitle{Well-definedness of $H_k$: Powell-type regularisation}
	To fix the singularity of $B_k$ we use powell-type regularisation.
%	\begin{center}
	\begin{algorithm}[H]
	\caption{AA-I with Powell-type regularisation}\label{alg:aa1-p}
	\SetKwInOut{Input}{Input}
	\color{gray}
s	\Input{$x^0\in\R^n$, $f\colon\R^n\to\R^n$ and ${\black\bartheta\in(0,1)}$.}
	\BlankLine
	Set $B_0=\Id$ and $x_1=f\brk{x_0}$.
	
	\For{$k=0,1,\dots$}{
		Set $g_k= g(x_{k})$,
		$s_{k-1}= x_k-x_{k-1}$ and
		$y_{k-1}= g_k-g_{k-1}$.
		
		Set $\hs_{k-1}= s_{k-1}-\sum_{i=0}^{k-2}\frac{\hs_i^\top s_{k-1}}{\norm{\hs_i}^2}s_i$.
		
		{\black
		Choose $\theta_{k-1}$ in dependence of $\bartheta$.
		
		Set $\tiy_{k-1}=\theta_{k-1}y_{k-1}-(1-\theta_{k-1})g_{k-1}$.
		}
		
		Set $B_k = B_{k-1}+\frac{\brk*{{\black\tiy_{k-1}}-B_{k-1}s_{k-1}}\hs_{k-1}^\top}{\hs_{k-1}^\top s_{k-1}}$.
		
		Set $H_k=B_k^{-1}$.
		
		Set $x_{k+1}= x_k-H_kg_k$.
		
	}
	\end{algorithm}	
%	\end{center}
\end{frame}

%\begin{frame}
%	One can obtain
%	\begin{lemma}[Powell-type regularisation]
%		If $B_k$ is well-defined in algorithm \ref{alg:aa1-p} we have that $B_k$ is invertible and $$\abs{\det B_k}\geq \bartheta^k\,.$$
%	\end{lemma}
%	\begin{proof}
%		See \cite[Lemma 2]{ZhaAA}.
%	\end{proof}
%\end{frame}

\subsection{Well-definedness of $H_k$, memory usage: Restarting iteration}
\begin{frame}
	\frametitle{Well-definedness of $H_k$, memory usage: Restarting iteration}
	If $\hs_k=0$ the update
	\begin{align*}
		B_{k} = B_{k-1}+\frac{\brk{\tiy_{k-1}-B_{k-1}s_{k-1}}\hs_{k-1}^\top}{\hs_{k-1}^\top s_{k-1}}
	\end{align*}
	is ill-defined. This occurs in algorithm \ref{alg:aa1-p} e.g.\ for $k>n$ as then $\hs_k=0$ by linear dependence.
	Hence we restart the algorithm with $x_k$ as the new starting point if
	\begin{itemize}
		\item $k=m+1$ for some fixed $m\in\N$ or
		\item $\norm{\hs_{k-1}}<\tau\norm{s_{k-1}}$ for some fixed $\tau\in(0,1)$.
	\end{itemize}
	It can be shown that $B_k$ is then well-defined.
\end{frame}

\begin{frame}
%	\IncMargin{1em}
	\begin{algorithm}[H]
	\caption{AA-I with Powell-type regularisation and Restarting}\label{alg:aa1-pr}
	\SetKwInOut{Input}{Input}
	\color{gray}
	\Input{$x^0\in\R^n$, $f\colon\R^n\to\R^n$, {\black$m \in\N$ }and $\bartheta{, \black\tau}\in(0,1)$}
	\BlankLine
	Set $B_0=\Id$, $x_1=f\brk{x_0}$ and {\black $m_0 = 0$}.
	
	\For{$k=0,1,\dots$}{
		Set $g_k= g(x_{k})$,
		{\black $m_k= m_{k-1}+1$}, 
		$s_{k-1}= x_k-x_{k-1}$ and
		$y_{k-1}= g_k-g_{k-1}$.
		
		Set $\hs_{k-1}= s_{k-1}-\sum_{i=k-m_k}^{k-2}\frac{\hs_i^\top s_{k-1}}{\norm{\hs_i}^2}s_i$.
		
		{\black
		\If{$m_k=m+1$ or $\norm{\hs_{k-1}}<\tau\norm{s_{k-1}}$}{
			Set $m_k=0$, $\hs_{k-1}= s_{k-1}$ and $B_{k-1}=\Id$.
		}
		}
		Choose $\theta_{k-1}$ in dependence of $\bartheta$.
		
		Set $\tiy_{k-1}=\theta_{k-1}y_{k-1}-(1-\theta_{k-1})g_{k-1}$.
		
		Set $B_k = B_{k-1}+\frac{\brk*{\tiy_{k-1}-B_{k-1}s_{k-1}}\hs_{k-1}^\top}{\hs_{k-1}^\top s_{k-1}}$.
		
		Set $H_k=B_k^{-1}$.
		
		Set $x_{k+1}= x_k-H_kg_k$.
		
	}
	\end{algorithm}
\end{frame}

%\begin{frame}
%	\newconstant{BkUpperBound}
%	\begin{lemma}[Restarting iteration]
%		In algorithm \ref{alg:aa1-pr} we have that $B_k$ is well-defined and there exists a constant $\useconstant{BkUpperBound}=\useconstant{BkUpperBound}(m,\bartheta,\tau)>0$ such that
%		\begin{align*}
%			\norm{B_k}\leq \useconstant{BkUpperBound}\,.
%		\end{align*}
%	\end{lemma}
%	\begin{proof}
%		See \cite[Lemma 3]{ZhaAA}.
%	\end{proof}
%\end{frame}

\begin{frame}
	One can then show
	\newconstant{upperHk}
	\begin{lemma}[bound on $\norm{H_k}_2$]
		In algorithm \ref{alg:aa1-pr} we have that $H_k$ is well-defined and there exists a constant $\useconstant{upperHk}=\useconstant{upperHk}(m,n, \bartheta,\tau)>0$ such that
		\begin{align*}
			\norm{H_k}_2\leq \useconstant{upperHk}\,.
		\end{align*}
	\end{lemma}
	\begin{proof}
		See \cite[Corollary 4]{ZhaAA}.
	\end{proof}
\end{frame}

\begin{frame}
	\centering
	\scalebox{0.9}{
	\input{../Figures/Diagram_001}
	}
\end{frame}

\subsection{Computational efficiency: Rank-1 update for $H_k$}

\begin{frame}
	\frametitle{Computational efficiency: Rank-1 update for $H_k$}
	From the Sherman-Morrison formula one can obtain
	\begin{proposition}[Rank-1 update for $H_k$]
		We have
		\begin{align*}
			H_{k} = H_{k-1}+\frac{\brk*{s_{k-1}-H_{k-1}y_{k-1}}\hs_{k-1}^\top H_{k-1}}{\hs_{k-1}^\top H_{k-1}y_{k-1}}
		\end{align*}
	\end{proposition}
\end{frame}


\begin{frame}
%	\IncMargin{1em}
	\begin{algorithm}[H]
	\caption{AA-I with Powell-type regularisation and Restarting}\label{alg:aa1-pr}
	\SetKwInOut{Input}{Input}
	\color{gray}
	\Input{$x^0\in\R^n$, $f\colon\R^n\to\R^n$, {\black$m \in\N$ }and $\bartheta{, \black\tau}\in(0,1)$}
	\BlankLine
	Set $H_0=\Id$, $x_1=f\brk{x_0}$ and {\black $m_0 = 0$}.
	
	\For{$k=0,1,\dots$}{
		Set $g_k= g(x_{k})$,
		$m_k= m_{k-1}+1$, 
		$s_{k-1}= x_k-x_{k-1}$ and
		$y_{k-1}= g_k-g_{k-1}$.
		
		Set $\hs_{k-1}= s_{k-1}-\sum_{i=k-m_k}^{k-2}\frac{\hs_i^\top s_{k-1}}{\norm{\hs_i}^2}s_i$.
		
		\If{$m_k=m+1$ or $\norm{\hs_{k-1}}<\tau\norm{s_{k-1}}$}{
			Set $m_k=0$, $\hs_{k-1}= s_{k-1}$ and $H_{k-1}=\Id$.
		}
		
		Choose $\theta_{k-1}$ in dependence of $\bartheta$.
		
		Set $\tiy_{k-1}=\theta_{k-1}y_{k-1}-(1-\theta_{k-1})g_{k-1}$.
		
		{\black
		Set $H_k = H_{k-1}+\frac{(s_{k-1}-H_{k-1}\tiy_{k-1})\hs_{k-1}^\top H_{k-1}}{\hs_{k-1}^\top H_{k-1}\tiy_{k-1}}$.
		}
		
		Set $x_{k+1}= x_k-H_kg_k$.
		
	}
	\end{algorithm}
\end{frame}

\subsection{Convergence: Safeguarding steps}
\begin{frame}
	\frametitle{Convergence: Safeguarding steps}
	To guarantee the decrease in $\norm{g_k}$ one can interleave the AA-I steps with Krasnosel'skii-Mann (KM) steps which are given by
	\begin{align*}
		x_{k+1}= (1-\alpha)x_k +\alpha f_k
	\end{align*}
	for some fixed $\alpha\in(0,1)$.
\end{frame}

\SetAlFnt{\scriptsize}

\begin{frame}
%	\IncMargin{1em}
	\begin{algorithm}[H]
	\caption{AA-I with Powell-type regularisation, restarting and safeguarding}\label{alg:aa1-prs}
	\SetKwInOut{Input}{Input}
	\color{gray}
	\Input{$x^0\in\R^n$, $f\colon\R^n\to\R^n$,$m \in\N$, $\bartheta, \tau, {\black\alpha}\in(0,1)$ and {\black safe-guarding constants $D,\e>0$}}
	\BlankLine
	Set $H_0=\Id$, $x_1={\black\tix_1=f\brk{x_0}}$, $m_0 = {\black n_{AA}=0}$ and ${\black\barU=\norm{g_0}_2}$.
	
	\For{$k=0,1,\dots$}{
		Set $g_k= g(x_{k})$,
		$m_k= m_{k-1}+1$, 
		$s_{k-1}= {\black\tix_k}-x_{k-1}$ and
		$y_{k-1}= g({\black\tix_k})-g_{k-1}$.
		
		Set $\hs_{k-1}= s_{k-1}-\sum_{i=k-m_k}^{k-2}\frac{\hs_i^\top s_{k-1}}{\norm{\hs_i}^2}s_i$.
		
		\If{$m_k=m+1$ or $\norm{\hs_{k-1}}<\tau\norm{s_{k-1}}$}{
			Set $m_k=0$, $\hs_{k-1}= s_{k-1}$ and $H_{k-1}=\Id$.
		}
		Choose $\theta_{k-1}$ in dependence of $\bartheta$.
		
		Set $\tiy_{k-1}=\theta_{k-1}y_{k-1}-(1-\theta_{k-1})g_{k-1}$.
		
		Set $H_k = H_{k-1}+\frac{(s_{k-1}-H_{k-1}\tiy_{k-1})\hs_{k-1}^\top H_{k-1}}{\hs_{k-1}^\top H_{k-1}\tiy_{k-1}}$ and $\tix_{k+1}= x_k-H_kg_k$.
		
		{\black
		\uIf{$\norm{g_k}\leq D\barU(n_{AA}+1)^{-(1+\e)}$}{
			Set $x_{k+1}=\tix_{k+1}$ and $n_{AA}= n_{AA}+1$.
		}
		\Else{
			Set $x_{k+1}= (1-\alpha)x_k +\alpha f_k$.
		}
		}
	}
	\end{algorithm}
\end{frame}

\SetAlFnt{\normalsize}

\section{Convergence result}

\begin{frame}
	\frametitle{Convergence result}
	\begin{theorem}[Convergence]
		Let $x_k$ be generated by algorithm \ref{alg:aa1-prs} then $x_k$ converges to a fixed point of $f$.
	\end{theorem}
\end{frame}

%\begin{frame}
%	\tikzstyle{lemma} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, draw=black]
%	\tikzstyle{arrow} = [thick,->,>=stealth]
%	 
%	\begin{tikzpicture}[node distance=2cm]
%		\node (restartIteration) [lemma, text width=4cm] {Lemma (Restarting Iteration)};
%		\node (Bk_welldefined) [lemma, below of=restartIteration] {$B_{k+1}$ well-defined};
%		\node (Bk_invertible) [lemma, below of=Bk_welldefined] {$B_{k+1}$ invertible};
%		\node (powell) [lemma, right of=restartIteration, xshift=4cm, text width=4cm] {Lemma (Powell-type regularisation)};
%		\node (Hk_bound) [lemma, below of=powell] {Lemma(Bound of $H_k$)};
%	\end{tikzpicture}
%\end{frame}

\begin{frame}
	\begin{proofs}[Proof, strategy]
	\vspace*{1cm}
	\centering
	\scalebox{0.9}{
	\input{../Figures/Diagram_002}
	}
	\end{proofs}
\end{frame}
\subsection{Proof, part 1}
\begin{frame}
	\begin{proofs}[Proof, part 1]
	The proof follows \cite[Theorem 6]{ZhaAA}.
	We partition $\N=K_{AA}\sqcup K_{KM}$ where $K_{AA}=\brk[c]{k_0,k_1,\dots}$ denote the indices $k$ where the algorithm chose an AA-step (a) and $K_{KM}=\brk[c]{l_0,l_1,\dots}$ where the algorithm chose a KM-step (b).
	
	\begin{center}
	\begin{algorithm}[H]
		\uIf{$\norm{g_k}\leq D\barU(n_{AA}+1)^{-(1+\e)}$}{
			Set $x_{k+1}=\tix_{k+1}$ and $n_{AA}= n_{AA}+1$. \brkcomment*[r]{a}
		}
		\Else{
			Set $x_{k+1}= (1-\alpha)x_k +\alpha f_k$. \brkcomment*[r]{b}
		}
	\caption{The two cases for $x_{k+1}$.}
	\end{algorithm}
	\end{center}
	\end{proofs}
\end{frame}


\begin{frame}
	\begin{proofs}[Proof, part 1 (cont.).]
	Let $y$ be a fixed point. We distinguish
	\begin{description}
		\item[case (a)]
		$k\in K_{AA}$ then
		\newconstant{CDU}
		\begin{equation}
		\begin{aligned}
			\norm{x_{k+1}-y}&\leq \norm{x_k-y}+\norm{H_kg_k} \\
			&\leq \norm{x_k-y}+\useconstant{upperHk}\norm{g_k} \\
			&\leq \norm{x_k-y}+\useconstant{CDU}(k+1)^{-(1+\e)}
			\label{eq:16}
		\end{aligned}
		\end{equation}
		\item[case (b)]
		$k\in K_{KM}$ then as $f$ is nonexpansive (motivate this)
		\begin{align}
			\norm{x_{k+1}-y}^2\leq \norm{x_k-y}^2-\alpha(1-\alpha)\norm{g_k}^2
			\label{eq:17}
		\end{align}
	\end{description}
	Hence in any case
	\newconstant{E}
	\begin{align*}
		\norm{x_k-y}
		\leq \norm{x_0-y}+\useconstant{CDU}\sum_k(k+1)^{-(1+\e)}
		= \useconstant{E}<\infty\,.
	\end{align*}
	\end{proofs}
\end{frame}


\begin{frame}
	\begin{proofs}[Proof, part 1 (cont.).]
	It then follows that
	\begin{equation}
	\begin{aligned}
		a_{k+1} &= \norm{x_{k+1}-y}^2\stackrel{\eqref{eq:16}, \eqref{eq:17}}{\leq}\brk*{\norm{x_k-y}+\useconstant{CDU}(k+1)^{-(1+\e)}}^2 \\
		&\leq \underbrace{\norm{x_k-y}^2}_{=a_k}+\underbrace{\useconstant{CDU}^2(k+1)^{-2(1+\e)}+2\useconstant{CDU}\overbrace{\norm{x_k-y}}^{\leq \useconstant{E}}(k+1)^{-(1+\e)}}_{=b_k} \\
		&= a_k+b_k
	\end{aligned}
	\label{eq:19}
	\end{equation}
	and hence
	\begin{align*}
		\alpha(1-\alpha)\sum_i\norm{g_{l_i}}^2
		\stackrel{\eqref{eq:17}}{\leq} \sum_i a_{l_i}-a_{l_i+1}
		\stackrel{\eqref{eq:19}}{\leq}a_0+\sum_k b_k
		<\infty
	\end{align*}
	We therefore have $\lim_i\norm{g_{l_i}}=0$. It also follows from $\norm{g_{k_i}}\leq D\barU (i+1)^{-(1+\e)}$ that $\lim_i\norm{g_{k_i}}=0$. Thus indeed $\lim_k\norm{g_k}=0$.
	\end{proofs}
\end{frame}

\subsection{Proof, part 2}
\begin{frame}
	\begin{proofs}[Proof, part 2.]
	Let now $n_j$ and  $N_j\geq n_j$ be such that
	\begin{align*}
		a_{n_j}\xrightarrow{j\to\infty}\liminf_ka_k=\underline{a} \\
		a_{N_j}\xrightarrow{j\to\infty}\limsup_ka_k=\overline{a}	
	\end{align*}
	Then it follows that
	\begin{align*}
		\overline{a}-\underline{a}
		\xleftarrow{n_j\to\infty}\overline{a}-a_{n_j}
		\xleftarrow{N_j\to\infty}a_{N_j}-a_{n_j}
		= \sum_{k=n_j}^{N_j-1}a_{k+1}-a_k
		\stackrel{\eqref{eq:19}}{\leq} \sum_{k=n_j}^\infty b_k
		\xrightarrow{n_j\to\infty}0
	\end{align*}
	so
	\begin{align*}
		\limsup_ka_k=\overline{a}\leq \underline{a}=\liminf_ka_k
	\end{align*}
	and thus $a_k=\norm{x_k-y}$ converges to some $a$.
	\end{proofs}
\end{frame}

\subsection{Proof, part 3}

\begin{frame}
	\begin{proofs}[Proof, part 3.]
	Let $k_j$ and $l_j$ be convergent subsequences of $x_k$ convergent against $y_1$ and $y_2$ respectively. Since by continuity of $g$
	\begin{align*}
		\norm{g(y_1)}=\lim_j\norm{g(x_{k_j})}\stackrel{\text{part 1}}{=}0
	\end{align*}
	we have that $y_1$ is a fixed point and $y_2$ too.
	Now by part 2
	\begin{align*}
		\norm{y_1} 
		\xleftarrow{j\to\infty} \norm{x_{k_j}}^2
		= \norm{x_{k_j}-y}^2-\norm{y}^2+2y^\top x_{k_j}
		\xrightarrow{j\to\infty} a-\norm{y}^2+2y^\top y_1
	\end{align*}
	and analogously for $y_2$. Thus
	\begin{align*}
		\norm{y_i}=a-\norm{y}^2+2y^\top y_i
	\end{align*}
	which implies
	\begin{align*}
		2y^\top(y_1-y_2) = \norm{y_1}^2-\norm{y_2}^2\,.
	\end{align*}
	\end{proofs}
\end{frame}


\begin{frame}
	\begin{proofs}[Proof, part 3 (cont.).]
	It then follows from 
	\begin{align*}
		2y^\top(y_1-y_2) = \norm{y_1}^2-\norm{y_2}^2
	\end{align*}
	with $y=y_i$ that
	\begin{align*}
		y_1^\top(y_1-y_2) = y_2^\top(y_1-y_2)
	\end{align*}
	and further
	\begin{align*}
		(y_1-y_2)^\top(y_1-y_2) = 0
	\end{align*}
	and thus $y_1=y_2$. We have shown that two convergent subsequences have the same limit and hence $x_k$ is convergent and the limit must be a fixed point of $f$.
	\end{proofs}
\end{frame}


\section{Numerical experiments}

%\subsection{Regularised logistic regression}

%\begin{frame}
%	\frametitle{Regularised logistic regression}
%	We take $x\in\R^{2000\times 500}$, $y\in\R^{2000}$ from the UCI Madelon dataset \cite{MadDat}. The aim is to minimise
%	\begin{align*}
%		F(\theta) = \frac{1}{2000}\sum_{i}\log\brk{1+\sum_jy_ix_{ij}\theta_j}+\frac{\lambda}{2}\norm{\theta}^2
%	\end{align*}
%	with gradient descent, i.e.\
%	\begin{align*}
%		f\colon \R^{500}\to\R^{500}, \quad \theta\mapsto\theta-\alpha\nabla F(\theta)
%	\end{align*}
%	for some $\alpha$.
%\end{frame}
%
%\begin{frame}
%	\begin{figure}
%		\centering
%		{\scriptsize
%		\input{../Plots/method_comparison_GD.pgf}
%		}
%		\caption{Residual norms for the logistic regression problem.}
%	\end{figure}
%\end{frame}
%
%\begin{frame}
%	\begin{figure}
%		\centering
%		{\scriptsize
%		\input{../Plots/memory_comparison_GD.pgf}
%		}
%		\caption{Residual norms for the logistic regression problem.}
%	\end{figure}
%\end{frame}
%
%\subsection{Facility location}
%
%\begin{frame}
%	\frametitle{Facility location}
%	The aim is to minimise
%	\begin{align*}
%		F\colon \R^{300}\to\R, \quad y\mapsto \sum_{i=1}^{500}\norm{y-c_i}
%	\end{align*}
%	for $c_i\in\R^{300}$ with sparsity $0.01$. This can lead to the formulation
%	\begin{align*}
%		\tilde{f}\colon \R^{500\times 300}\to\R^{500\times 300}, \quad
%		z\mapsto \brk*{z_i+2\tikzmark{avg_x_dest}{\avg{x}}-\tikzmark{xi_dest}{x_i}-\avg{z}}_i
%	\end{align*}
%	with
%	\begin{align*}
%		\tikzmark{avg_x_source}{\avg{x}} = \frac{1}{500}\sum_ix_i \qquad \tikzmark{xi_source}{x_i} = \tikzmark{prox_dest}{\prox_{\norm{\cdot}}}\brk*{z_i+c_i}-c_i
%	\end{align*}
%	and
%	\begin{align*}
%		\tikzmark{prox_source}{\prox_{\norm{\cdot}}}(v) = \brk*{1-\frac{1}{\norm{v}}}_+v\,.
%	\end{align*}
%	\tikzset{external/export=false}
%	\begin{tikzpicture}[remember picture, overlay, node distance = 0.5cm]
%		\draw[,->,thick] (avg_x_source) to [in=-125,out=60] (avg_x_dest);
%		\draw[,->,thick] (xi_source) to [in=-90,out=60] (xi_dest);
%%		\draw[,->,thick] (prox_source) to [in=-90,out=60] (prox_dest);
%	\end{tikzpicture}%
%\end{frame}
%
%
%\begin{frame}
%	\begin{figure}
%		\centering
%		{\scriptsize
%		\input{../Plots/method_comparison_CO.pgf}
%		}
%		\caption{Residual norms for the facility location problem.}
%	\end{figure}
%\end{frame}
%
%
%\begin{frame}
%	\begin{figure}
%		\centering
%		{\scriptsize
%		\input{../Plots/memory_comparison_CO.pgf}
%		}
%		\caption{Residual norms for the facility location problem.}
%	\end{figure}
%\end{frame}

\subsection{Elastic net regression}

\begin{frame}
	\frametitle{Elastic net regression}
	The aim is to find a fixed point of
	\begin{align*}
		f\colon\R^{1000}\to\R^{1000}, \quad x\mapsto S_{\alpha\mu/2}\brk*{x-\alpha\brk*{A^\top(Ax-b)+\frac{\mu}{2}x}}
	\end{align*}
	with shrinkage operator
	\begin{align*}
		S_\kappa(x) = \brk*{\sgn(x_i)\brk*{\abs{x_i}-\kappa}_+}_i
	\end{align*}
	and $A\in\R^{500\times 1000}$, $b\in\R^{500}$ and some $\alpha, \mu\in\R$ as in \cite{ZhaAA}.
\end{frame}


\begin{frame}
	\begin{figure}
		\centering
		{\scriptsize
		\input{../Plots/method_comparison_ISTA.pgf}
		}
		\caption{Residual norms for the elastic net regression problem.}
	\end{figure}
\end{frame}

\begin{frame}
	\begin{figure}
		\centering
		{\scriptsize
		% \tiny
		\input{../Plots/memory_comparison_ISTA.pgf}
		}
		\caption{Residual norms for the elastic net regression problem.}
	\end{figure}
\end{frame}
\subsection{Markov decision process}

\begin{frame}
	\frametitle{Markov decision process}
	Our aim is to find a fixed point of the Bellman operator
	\begin{align*}
		f\colon\R^{1000}\to\R^{1000}, \quad x\mapsto \brk*{\max_{a} \brk3{ R(s,a)+\gamma \sum_{s'}P(s,a,s')x_{s'}}}_{s}
	\end{align*}
	with some $R\in\R^{300\times 200}$, $P\in\R^{300\times 200\times 300}$, $\gamma\in\R$ as in \cite{ZhaAA}.
\end{frame}


\begin{frame}
	\begin{figure}
		\centering
		{\scriptsize
		\input{../Plots/method_comparison_VI.pgf}
		}
		\caption{Residual norms for the Markov decision process problem.}
	\end{figure}
\end{frame}

\begin{frame}
	\begin{figure}
		\centering
		{\scriptsize
		% \tiny
		\input{../Plots/memory_comparison_VI.pgf}
		}
		\caption{Residual norms for the Markov decision process problem.}
	\end{figure}
\end{frame}

\section{Summary}

\begin{frame}
	\frametitle{Summary}
	\begin{itemize}
		\item The aim is to find a fixed point of a non-expansive $f$ where
		\begin{itemize}
			\item the dimension is large
			\item $f$ is expensive to evaluate, noisy and the gradient is a mystery
		\end{itemize}
		\item The main idea is to generalise the fixed point iteration with $x_{k+1}=\sum_i\alpha_if_i$ for some clever choice of $\alpha=\alpha^k\in\R^{k+1}$.
		\item Modifications of the AA-I algorithm:
		\begin{itemize}
			\item Powell-type regularisation \textrightarrow well-definedness
			\item Restarting iteration \textrightarrow well-definedness, limited memory
			\item Safeguarding steps \textrightarrow convergence
			\item Rank-1 update for $H_k$ \textrightarrow matrix-free
		\end{itemize}
		\item Convergence result
		\item Numerical experiments: AA-I with the modifications often outperforms the fixed point iteration.
	\end{itemize}
\end{frame}

\section*{Sources}

\nocite{*}

\begin{frame}
	\frametitle{Main source}
%	\bibliographystyle{plain}
%	\bibliography{bibliography}
	\printbibliography[title={Main source},keyword={main}]
\end{frame}

\begin{frame}
	\frametitle{Other sources}
	\printbibliography[keyword={secondary}, title={Other sources}]
\end{frame}

\begin{frame}
	\frametitle{Image sources}
	\printbibliography[title={Image sources}, keyword={image}]
\end{frame}


\begin{frame}[plain]
	\begin{center}
		\Large{{Thank you for your attention.}}
	\end{center}
\end{frame}

%\frame[plain]

\end{document}

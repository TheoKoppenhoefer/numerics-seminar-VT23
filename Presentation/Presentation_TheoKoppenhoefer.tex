%

\input{../Latex_Templates/Preamble_Presentation}

%%%%% TITLE PAGE

\subject{, VT23}
\title{%Junzi Zhang, Brendan O'Donoghue, Stephen Boyd: 
Globally Convergent Type-I Anderson Acceleration for Non-Smooth Fixed-Point Iterations}
%\subtitle{}
\author{Theo KoppenhÃ¶fer}
\date{Lund \\[1ex] \today}


%\SetAlFnt{\small}
\addbibresource{bibliography.bib}



\begin{document}

\frame[plain]



% Frame 2
\frame[plain]{\titlepage}

% Frame 3
\frame[plain]{ \frametitle{Table of contents} \tableofcontents }

\section{The problem setting}
\begin{frame}
	\frametitle{The problem setting}
	\begin{problem}[find fixed point]
		Find a fixed point $x\in\R^n$ of $f\colon\R^n\to\R^n$, i.e.\ $x=f(x)$.
	\end{problem}
	or equivalently
	\begin{problem}[find zero]
		Find a zero $x\in\R^n$ of $g=\Id-f$, i.e.\ $0=g(x)$.
	\end{problem}
	We also assume
	\begin{itemize}
		\item $f$ is nonexpansive, i.e.\ $\norm{f(x)-f(y)}\leq\norm{x-y}$
		\item $n$ is large \textrightarrow matrix-free
		\item $\nabla f$ is unknown \textrightarrow no Newton
		\item cost of evaluation of $f$ is high \textrightarrow no line search
		\item noisy problem \textrightarrow no finite difference derivatives
	\end{itemize}
\end{frame}

\section{Motivation of AA-I}
\subsection{Fixed point iteration}
\begin{frame}
	\frametitle{Fixed point iteration}
	To keep things simple we try

	\begin{figure}
	\begin{algorithm}[H]
	\caption{Fixed point iteration (original)}
	\SetKwInOut{Input}{Input}
	
	\Input{Initial value $x_0\in\R^n$ and function $f\colon\R^n\to\R^n$.}
	\BlankLine
	\For{$k=0,1,\dots$}{
		Set $x_{k+1} =f\brk*{x_k}$.
	}
	\end{algorithm}
	\end{figure}
\end{frame}


\begin{frame}
	\begin{block}{This works, but ...} \vspace{1cm}
		\includegraphics[scale=0.25]{../Figures/turtle}	
	\end{block}
\end{frame}
\begin{frame}
	\begin{block}{We want to be like...} \vspace{2cm}
		\includegraphics[scale=0.17]{../Figures/hare}
	\end{block}
\end{frame}

\subsection{General AA}
\begin{frame}
	\frametitle{General AA}
	We may as well use the information gained from previous evaluations. If we form a weighted average we get
	
	\begin{figure}
	\begin{algorithm}[H]
	\caption{General AA (Anderson Acceleration)}
	\SetKwInOut{Input}{Input}
	\color{gray}
	
	\Input{$x_0\in\R^n$ and $f\colon\R^n\to\R^n$.}
	\BlankLine
	\For{$k=0,1,\dots$}{
		{\color{black}
		Set $f_k =f\brk*{x_k}$.
		
		Choose $\alpha = \alpha^k\in \R^{k}$ such that $\sum_i\alpha_i=1$.
	  
		Set $x_{k+1} = \sum_i \alpha_if_{i}$.
		}
	}
	\end{algorithm}
	\end{figure}
\end{frame}

\subsection{AA-II}
\begin{frame}
	\frametitle{AA-II}
%	\IncMargin{1em}
	Since finding a fixed point of $f$ is equivalent to finding a zero of $g=\Id-f$ we have the ansatz
	
	\begin{figure}
	\begin{algorithm}[H]
	\caption{AA-II}
	\SetKwInOut{Input}{Input}
	\color{gray}
	\Input{$x_0\in\R^n$ and $f\colon\R^n\to\R^n$.}
	\BlankLine
	\For{$k=0,1,\dots$}{
	  Set $f_k =f\brk*{x_k}$.
	  
	  {\color{black}Set $g_k = x_k-f_k$.}
	  
	  Choose $\alpha\in \R^{k}$ such that $\sum_i\alpha_i=1$ {\color{black} and such that $\alpha$ minimises $\norm{\sum_i\alpha_ig_i}_2$}.
	  
	 Set $x_{k+1} = \sum_i \alpha_if_{i}$.
	}
	\end{algorithm}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Rewriting AA-II}
	Setting 
	\begin{align*}
		\alpha=\vect{\gamma_0 \\ \gamma_1-\gamma_0 \\ \vdots \\ \gamma_{k}-\gamma_{k-1} \\ 1-\gamma_k}
		\text{ and }
		Y_k = \begin{bmatrix}
			& & \\
			g_{1}-g_0 & \cdots & g_{k}-g_{k-1} \\
			& &
		\end{bmatrix} \in\R^{n\times k}
	\end{align*}
	one obtains the least squares problem
	\begin{align*}
		\min_{\substack{\alpha\in\R^{k+1} \\ \sum_i\alpha_i=1}}\norm2{\sum_i\alpha_ig_i}
		= \min_{\gamma \in\R^k}\norm{g_k-Y_k\gamma}
	\end{align*}
	which is solved by
	\begin{align*}
		\gamma= \gamma^k = \brk*{Y_k^\top Y_k}^{-1}Y_k^\top g_k\,.
	\end{align*}
\end{frame}

\begin{frame}
	If we now set
	\begin{align*}
		S_k = \begin{bmatrix}
			& & \\
			x_1-x_0 & \cdots & x_{k}-x_{k-1} \\
			& & \\
		\end{bmatrix} \in\R^{n\times k}
	\end{align*}
	we see that
	\begin{align*}
		S_k -Y_k &= \begin{bmatrix}
			& & \\
			x_1-x_0-g_0+g_1 & \cdots & x_{k}-x_{k-1}-g_{k}+g_{k-1} \\
			& & \\
		\end{bmatrix} \\
		&= \begin{bmatrix}
			& & \\
			f_1-f_0 & \cdots & f_k-f_{k-1} \\
			& & \\
		\end{bmatrix}
	\end{align*}
\end{frame}

\begin{frame}
	and hence
	\begin{align*}
		x_{k+1} &= \sum_i\alpha_if(x_i) \\
		&= f_k -(S_k-Y_k)\gamma \\ \\
		&\tikzmark{def_gamma}{=} x_k -\underbrace{\brk*{\Id+(S_k-Y_k)\brk*{Y_k^\top Y_k}^{-1}Y_k^\top}}_{=H_k}g_k \\
		&= x_k -H_kg_k\,.
	\end{align*}
	\tikzset{external/export=false}
	\begin{tikzpicture}[remember picture, overlay, node distance = 0.3cm]
		\node[,text width=10cm] (def_gamma_descr) [above right=0.2cm and 0.1cm of def_gamma]{$f_k=x_k-g_k$ and $\gamma=\brk*{Y_k^\top Y_k}^{-1}Y_k^\top$};
		\draw[,->,thick] (def_gamma_descr) to [in=90,out=180] (def_gamma);
	\end{tikzpicture}%
\end{frame}

\begin{frame}
	We thus have the reformulation
	\begin{figure}
%	\IncMargin{1em}
	\begin{algorithm}[H]
	\caption{AA-II (reformulated)}
	\SetKwInOut{Input}{Input}
	\color{gray}
	\Input{$x_0\in\R^n$ and $f\colon\R^n\to\R^n$.}
	\BlankLine
	{\color{black}Set $x_1 =f(x_0)$.}
	
	\For{$k=0,1,\dots$}{
		Set $g_k= g(x_k)$.
		
		{\color{black}Construct $S_k$ from $x_0,\dots, x_k$ and $Y_k$ from $g_0,\dots,g_k$.
		
		Set $H_k = \Id +(S_k-Y_k)\brk*{Y_k^\top Y_k}^{-1}Y_k^\top$.
%		$s_{k-1}= x_k-x_{k-1}$ and
%		$y_{k-1}= g_k-g_{k-1}$.
		
		Set $x_{k+1}= x_k-H_kg_k$.}
	}
	\end{algorithm}
	\end{figure}
\end{frame}


\subsection{AA-I}
\begin{frame}
	\frametitle{AA-I}
	This is the form of a quasi-Newton-like method so one could expect $H_k$ to be an approximate inverse of $\nabla f(x_k)$. Indeed
	\begin{proposition}[Approximate inverse Jacobian]
		$H_k$ minimises $\norm{H_k-\Id}_F$ under the multisecant condition $H_kS_k=Y_k$.
	\end{proposition}
	From Broydens method we know that it is a good idea to approximate the Jacobian rather than its inverse.
	\begin{definition}[Approximate Jacobian]
		Let $B_k$ be minimiser of $\norm{B_k-\Id}_F$ under the condition $B_kY_k=S_k$.
	\end{definition}
	Analogously to AA-II we have
	\begin{align*}
		B_k = \Id+\brk*{Y_k-S_k}\brk*{S_k^\top S_k}^{-1}S_k^\top\,.
	\end{align*}
\end{frame}

\begin{frame}
	This yields the AA-I algorithm
	\begin{figure}
%	\IncMargin{1em}
	\begin{algorithm}[H]
	\caption{AA-I}
	\SetKwInOut{Input}{Input}
	\color{gray}
	\Input{$x_0\in\R^n$ and $f\colon\R^n\to\R^n$.}
	\BlankLine
	Set $x_1=f(x_0)$
	
	\For{$k=0,1,\dots$}{
		Set $g_k= g(x_k)$.
		
		Construct $S_k$ from $x_0,\dots, x_k$ and $Y_k$ from $g_0,\dots,g_k$.
				
		{\color{black}Set $B_k = \Id+\brk*{Y_k-S_k}\brk*{S_k^\top S_k}^{-1}S_k^\top$.
		
		Set $H_k = B_k^{-1}$.}
%		$s_{k-1}= x_k-x_{k-1}$ and
%		$y_{k-1}= g_k-g_{k-1}$.
		
		Set $x_{k+1}= x_k-H_kg_k$.
	}
	\end{algorithm}
	\end{figure}
\end{frame}

\begin{frame}
	Luckily for us we can save some computations by using the rank-1 update formula
	\begin{proposition}[Rank-1 update for $B_k$]
		We have
		\begin{align*}
			B_{k+1} = B_k+\frac{\brk*{y_k-B_ks_k}\hs_k^\top}{\hs_k^\top s_k}
		\end{align*}
		where $y_k = g_{k+1}-g_k$, $B_0=\Id$ and
		\begin{align*}
			\hs_k = s_k-\sum_{j=0}^{k-1}\frac{\hs_k^\top s_k}{\norm{\hs_k}^2}\hs_k
		\end{align*}
		is the Gram-Schmidt orthogonalisation of $s_k=x_{k+1}-x_k$.
	\end{proposition}
\end{frame}

\begin{frame}
	From the Sherman-Morrison formula it then follows that
	\begin{proposition}[Rank-1 update for $H_k$]
		We have
		\begin{align*}
			H_{k+1} = H_k+\frac{\brk*{s_k-H_ky_k}\hs_k^\top H_k}{\hs_k^\top H_ky_k}
		\end{align*}
		where $y_k = g_{k+1}-g_k$, $H_0=\Id$ and
		\begin{align*}
			\hs_k = s_k-\sum_{j=0}^{k-1}\frac{\hs_k^\top s_k}{\norm{\hs_k}^2}\hs_k
		\end{align*}
		is the Gram-Schmidt orthogonalisation of $s_k=x_{k+1}-x_k$.
	\end{proposition}
\end{frame}

\begin{frame}
	Taking everything together we obtain
	\begin{figure}
%	\IncMargin{1em}
	\begin{algorithm}[H]
	\caption{AA-I (rank-1 update)}
	\SetKwInOut{Input}{Input}
	\color{gray}
	\Input{$x_0\in\R^n$ and $f\colon\R^n\to\R^n$.}
	\BlankLine
	Set {\color{black}$H_0=\Id$} and $x_1=f\brk{x_0}$.
	
	\For{$k=0,1,\dots$}{
		Set $g_k= g(x_{k})$.
		
		{\color{black}
		Set $s_{k-1}= x_k-x_{k-1}$,
		$y_{k-1}= g_k-g_{k-1}$ and
		$\hs_{k-1}= s_{k-1}-\sum_{i=0}^{k-2}\frac{\hs_i^\top s_{k-1}}{\norm{\hs_i}^2}s_i$.
		
		Set $H_k = H_{k-1}+\frac{(s_{k-1}-H_{k-1}y_{k-1})s_{k-1}^\top H_{k-1}}{\hs_{k-1}^\top H_{k-1}y_{k-1}}$.}
		
		Set  $x_{k+1}= x_k-H_kg_k$.
		
	}
	\end{algorithm}
	\end{figure}
\end{frame}

\section{Modifications to AA-I}
\subsection{Powell-type regularisation}

\begin{frame}	
	\frametitle{Powell-type regularisation}
	Note that $B_k$ may be singular. To fix this set
	\begin{align*}
		\tiy_k=\theta_ky_k+(1-\theta_k)B_ks_k
	\end{align*}
	or equivalently
	\begin{align*}
		\tiy_k=\theta_ky_k+(1-\theta_k)B_ks_k
	\end{align*}
	where
	\begin{align*}
		\theta_k = \tikzmark{phi_dest}{\phi_{\bartheta}}(\tikzmark{eta_dest}{\eta_k})
	\end{align*}
	with
	\begin{align*}
		\tikzmark{phi_source}{\phi_{\bartheta}}(\eta) = \begin{cases}
			\frac{1-\sgn(\eta)\bartheta}{1-\eta} &\text{ if }\abs{\eta}<\bartheta \\
			1 &\text{ else }
		\end{cases} \quad\text{ and }\quad
		\tikzmark{eta_source}{\eta_k} = \frac{\hs_k^\top H_ky_k}{\norm{\hs_k}^2}
	\end{align*}	
	\tikzset{external/export=false}
	\begin{tikzpicture}[remember picture, overlay, node distance = 1cm]
		\draw[,->,thick] (phi_source) to [in=-90,out=90] (phi_dest);
		\draw[,->,thick] (eta_source) to [in=-90,out=90] (eta_dest);
	\end{tikzpicture}%
\end{frame}

\begin{frame}
	One can obtain
	\begin{lemma}[Powell-type regularisation]
		Let $s_k\in\R^n$, $B_0=\Id$, and inductively
		\begin{align*}
			B_{k+1} = B_k+\frac{\brk{\tiy_k-B_ks_k}\hs_k^\top}{\hs_k^\top s_k}
		\end{align*}
		with $\hs_k$ and $\tiy_k$ defined as before. If this is well-defined then $\abs{\det\brk{B_k}}\geq \theta^k>0$ and $B_k$ is invertible.
	\end{lemma}
	\begin{proof}
		See \cite[Lemma 2]{ZhaAA}.
	\end{proof}
\end{frame}

\begin{frame}
%	\IncMargin{1em}
	\begin{algorithm}[H]
	\caption{AA-I with Powell-like-regularisation}\label{alg:aa1-p}
	\SetKwInOut{Input}{Input}
	\color{gray}
s	\Input{$x^0\in\R^n$, $f\colon\R^n\to\R^n$ and ${\black\bartheta\in(0,1)}$.}
	\BlankLine
	Set $H_0=\Id$ and $x_1=f\brk{x_0}$.
	
	\For{$k=0,1,\dots$}{
		Set $g_k= g(x_{k})$,
		$s_{k-1}= x_k-x_{k-1}$ and
		$y_{k-1}= g_k-g_{k-1}$.
		
		Set $\hs_{k-1}= s_{k-1}-\sum_{i=0}^{k-2}\frac{\hs_i^\top s_{k-1}}{\norm{\hs_i}^2}s_i$.
		
		{\black
		Set $\eta_{k-1}= \frac{\hs_{k-1}^\top H_{k-1}y_{k-1}}{\norm{\hs_{k-1}}^2}$, 
		$\theta_{k-1}=\phi_{\bartheta}(\eta_{k-1})$
		and $\tiy_{k-1}=\theta_{k-1}y_{k-1}-(1-\theta_{k-1})g_{k-1}$.
		}
		
		Set $H_k = H_{k-1}+\frac{(s_{k-1}-H_{k-1}\tiy_{k-1})}{\hs_{k-1}^\top H_{k-1}\tiy_{k-1}}$ and $x_{k+1}= x_k-H_kg_k$.
		
	}
	\end{algorithm}
\end{frame}

\subsection{Restarting iteration}
\begin{frame}
	\frametitle{Restarting iteration}
	Note that
	\begin{align*}
		B_{k+1} = B_k+\frac{\brk{\tiy_k-B_ks_k}\hs_k^\top}{\hs_k^\top s_k}
	\end{align*}
	is ill-defined iff $\norm{\hs_k}^2=\hs_k^\top s_k=0$, i.e.\ $\hs_k=0$. This occurs in algorithm \ref{alg:aa1-p} for $k>n$ as then $\hs_k=0$ by linear dependence.
	If we restart the algorithm with $x_k$ as the new starting point if $k=m+1$ for some $m\in\N$ or $\norm{\hs_{k-1}}<\tau\norm{s_{k-1}}$ for some $\tau\in(0,1)$
	then 
	$$g_k\neq 0 \implies s_k=-B_kg_k\neq 0\implies \hs_k\neq 0\,.$$
\end{frame}

\begin{frame}
%	\IncMargin{1em}
	\begin{algorithm}[H]
	\caption{AA-I with Powell-like-regularisation and Restarting}\label{alg:aa1-pr}
	\SetKwInOut{Input}{Input}
	\color{gray}
	\Input{$x^0\in\R^n$, $f\colon\R^n\to\R^n$, {\color{black}$m \in\N$ }and $\bartheta{, \black\tau}\in(0,1)$}
	\BlankLine
	Set $H_0=\Id$, $x_1=f\brk{x_0}$ and {\color{black} $m_0 = 0$}.
	
	\For{$k=0,1,\dots$}{
		Set $g_k= g(x_{k})$,
		$m_k= m_{k-1}+1$, 
		$s_{k-1}= x_k-x_{k-1}$ and
		$y_{k-1}= g_k-g_{k-1}$.
		
		Set $\hs_{k-1}= s_{k-1}-\sum_{i=k-m_k}^{k-2}\frac{\hs_i^\top s_{k-1}}{\norm{\hs_i}^2}s_i$.
		
		{\black
		\If{$m_k=m+1$ or $\norm{\hs_{k-1}}<\tau\norm{s_{k-1}}$}{
			Set $m_k=0$, $\hs_{k-1}= s_{k-1}$ and $H_{k-1}=\Id$.
		}
		}
		Set $\eta_{k-1}= \frac{\hs_{k-1}^\top H_{k-1}y_{k-1}}{\norm{\hs_{k-1}}^2}$, 
		$\theta_{k-1}=\phi_{\bartheta}(\eta_{k-1})$
		and $\tiy_{k-1}=\theta_{k-1}y_{k-1}-(1-\theta_{k-1})g_{k-1}$.
		
		Set $H_k = H_{k-1}+\frac{(s_{k-1}-H_{k-1}\tiy_{k-1})}{\hs_{k-1}^\top H_{k-1}\tiy_{k-1}}$ and $x_{k+1}= x_k-H_kg_k$.
		
	}
	\end{algorithm}
\end{frame}

\begin{frame}
	\begin{lemma}[Restarting iteration]
		If we additionally choose $m_k$ by the rule above we have
		\begin{align*}
			\norm{B_k}\leq 3\brk*{\frac{1+\bartheta+\tau}{\tau}}^m-2\,.
		\end{align*}
	\end{lemma}
	\begin{proof}
		See \cite[Lemma 3]{ZhaAA}.
	\end{proof}
\end{frame}

\begin{frame}
	\begin{lemma}[bound on $\norm{H_k}_2$]
		In algorithm \ref{alg:aa1-pr} we have that
		\begin{align*}
			\norm{H_k}_2\leq \frac{1}{\bartheta^m}\brk*{3\brk*{\frac{1+\bartheta+\tau}{\tau}}^m-2}^{n-1}\,.
		\end{align*}
	\end{lemma}
	\begin{proof}
		This follows from Lemma (Restarting iteration) and Lemma (Powell-type regularisation).
	\end{proof}
\end{frame}

\begin{frame}
	\centering
	\input{../Figures/Diagram_001}
\end{frame}


\subsection{Safeguarding steps}
\begin{frame}
	\frametitle{Safeguarding steps}
	To guarantee the decrease in $\norm{g_k}$ one can interleave the AA-I steps with Krasnosel'skii-Mann steps which are given by
	\begin{align*}
		x_{k+1}= (1-\alpha)x_k +\alpha f(x_k)
	\end{align*}
	for some fixed $\alpha\in(0,1)$.
\end{frame}

\SetAlFnt{\scriptsize}

\begin{frame}
%	\IncMargin{1em}
	\begin{algorithm}[H]
	\caption{AA-I with Powell-like-regularisation, Restarting and Safeguarding}\label{alg:aa1-prs}
	\SetKwInOut{Input}{Input}
	\color{gray}
	\Input{$x^0\in\R^n$, $f\colon\R^n\to\R^n$,$m \in\N$, $\bartheta, \tau, {\color{black}\alpha}\in(0,1)$ and {\color{black}safe-guarding constants $D,\e>0$}}
	\BlankLine
	Set $H_0=\Id$, $x_1={\black\tix_1=f\brk{x_0}}$, $m_0 = {\color{black}n_{AA}=0}$ and ${\color{black}\barU=\norm{g_0}_2}$.
	
	\For{$k=0,1,\dots$}{
		Set $g_k= g(x_{k})$,
		$m_k= m_{k-1}+1$, 
		$s_{k-1}= {\black\tix_k}-x_{k-1}$ and
		$y_{k-1}= g({\black\tix_k})-g_{k-1}$.
		
		Set $\hs_{k-1}= s_{k-1}-\sum_{i=k-m_k}^{k-2}\frac{\hs_i^\top s_{k-1}}{\norm{\hs_i}^2}s_i$.
		
		\If{$m_k=m+1$ or $\norm{\hs_{k-1}}<\tau\norm{s_{k-1}}$}{
			Set $m_k=0$, $\hs_{k-1}= s_{k-1}$ and $H_{k-1}=\Id$.
		}
		Set $\eta_{k-1}= \frac{\hs_{k-1}^\top H_{k-1}y_{k-1}}{\norm{\hs_{k-1}}^2}$, 
		$\theta_{k-1}=\phi_{\bartheta}(\eta_{k-1})$
		and $\tiy_{k-1}=\theta_{k-1}y_{k-1}-(1-\theta_{k-1})g_{k-1}$.
		
		Set $H_k = H_{k-1}+\frac{(s_{k-1}-H_{k-1}\tiy_{k-1})}{\hs_{k-1}^\top H_{k-1}\tiy_{k-1}}$ and $\tix_{k+1}= x_k-H_kg_k$.
		
		{\color{black}
		\uIf{$\norm{g_k}\leq D\barU(n_{AA}+1)^{-(1+\e)}$}{
			Set $x_{k+1}=\tix_{k+1}$ and $n_{AA}= n_{AA}+1$.
		}
		\Else{
			Set $x_{k+1}= (1-\alpha)x_k +\alpha f(x_k)$
		}
		}
	}
	\end{algorithm}
\end{frame}

\section{Convergence result}

\begin{frame}
	\frametitle{Convergence result}
	\begin{theorem}[Convergence]
		Let $x_k$ be generated by algorithm \ref{alg:aa1-prs} then $x_k\xrightarrow{k\to\infty}x$ and $f(x)=x$ is a fixed point.
	\end{theorem}
\end{frame}

%\begin{frame}
%	\tikzstyle{lemma} = [rectangle, minimum width=3cm, minimum height=1cm, text centered, draw=black]
%	\tikzstyle{arrow} = [thick,->,>=stealth]
%	 
%	\begin{tikzpicture}[node distance=2cm]
%		\node (restartIteration) [lemma, text width=4cm] {Lemma (Restarting Iteration)};
%		\node (Bk_welldefined) [lemma, below of=restartIteration] {$B_{k+1}$ well-defined};
%		\node (Bk_invertible) [lemma, below of=Bk_welldefined] {$B_{k+1}$ invertible};
%		\node (powell) [lemma, right of=restartIteration, xshift=4cm, text width=4cm] {Lemma (Powell-type regularisation)};
%		\node (Hk_bound) [lemma, below of=powell] {Lemma(Bound of $H_k$)};
%	\end{tikzpicture}
%\end{frame}

\begin{frame}
	\centering
	\input{../Figures/Diagram_002}
\end{frame}


\section{Numerical experiments}

\subsection{Regularised logistic regression}

\begin{frame}
	\frametitle{Regularised logistic regression}
	We take $x\in\R^{2000\times 500}$, $y\in\R^{2000}$ from the UCI Madelon dataset \cite{MadDat}. The aim is to minimise
	\begin{align*}
		F(\theta) = \frac{1}{2000}\sum_{i}\log\brk{1+\sum_jy_ix_{ij}\theta_j}+\frac{\lambda}{2}\norm{\theta}^2
	\end{align*}
	with gradient descent, i.e.\
	\begin{align*}
		f\colon \R^{500}\to\R^{500}, \quad \theta\mapsto\theta-\alpha\nabla F(\theta)
	\end{align*}
	for some $\alpha$.
\end{frame}

\begin{frame}
	\begin{figure}
		\centering
		{\scriptsize
		\input{../Plots/method_comparison_GD.pgf}
		}
		\caption{Residual norms for the logistic regression problem.}
	\end{figure}
\end{frame}

\begin{frame}
	\begin{figure}
		\centering
		{\scriptsize
		\input{../Plots/memory_comparison_GD.pgf}
		}
		\caption{Residual norms for the logistic regression problem.}
	\end{figure}
\end{frame}

\subsection{Facility location}

\begin{frame}
	\frametitle{Facility location}
	The aim is to minimise
	\begin{align*}
		F\colon \R^{300}\to\R, \quad y\mapsto \sum_{i=1}^{500}\norm{y-c_i}
	\end{align*}
	for $c_i\in\R^{300}$ with sparsity $0.01$. This can lead to the formulation
	\begin{align*}
		\tilde{f}\colon \R^{500\times 300}\to\R^{500\times 300}, \quad
		z\mapsto \brk*{z_i+2\tikzmark{avg_x_dest}{\avg{x}}-\tikzmark{xi_dest}{x_i}-\avg{z}}_i
	\end{align*}
	with
	\begin{align*}
		\tikzmark{avg_x_source}{\avg{x}} = \frac{1}{500}\sum_ix_i \qquad \tikzmark{xi_source}{x_i} = \tikzmark{prox_dest}{\prox_{\norm{\cdot}}}\brk*{z_i+c_i}-c_i
	\end{align*}
	and
	\begin{align*}
		\tikzmark{prox_source}{\prox_{\norm{\cdot}}}(v) = \brk*{1-\frac{1}{\norm{v}}}_+v\,.
	\end{align*}
	\tikzset{external/export=false}
	\begin{tikzpicture}[remember picture, overlay, node distance = 0.5cm]
		\draw[,->,thick] (avg_x_source) to [in=-125,out=60] (avg_x_dest);
		\draw[,->,thick] (xi_source) to [in=-90,out=60] (xi_dest);
%		\draw[,->,thick] (prox_source) to [in=-90,out=60] (prox_dest);
	\end{tikzpicture}%
\end{frame}


\begin{frame}
	\begin{figure}
		\centering
		{\scriptsize
		\input{../Plots/method_comparison_CO.pgf}
		}
		\caption{Residual norms for the facility location problem.}
	\end{figure}
\end{frame}


\begin{frame}
	\begin{figure}
		\centering
		{\scriptsize
		\input{../Plots/memory_comparison_CO.pgf}
		}
		\caption{Residual norms for the facility location problem.}
	\end{figure}
\end{frame}

\subsection{Elastic net regression}

\begin{frame}
	\frametitle{Elastic net regression}
	Our aim is to minimise
	\begin{align*}
		F\colon \R^{1000}\to \R, \quad x\mapsto \frac{1}{2}\norm{Ax-b}^2+\mu\brk*{\frac{1}{4}\norm{x}^2+\frac{1}{2}\norm{x}_1}
	\end{align*}
	with $A\in\R^{500\times 1000}$, $b\in\R^{500}$ and some $\mu\in\R$. From the Iterative Shrinkage-Thresholding Algorithm one obtains
	\begin{align*}
		f\colon\R^{1000}\to\R^{1000}, \quad x\mapsto S_{\alpha\mu/2}\brk*{x-\alpha\brk*{A^\top(Ax-b)+\frac{\mu}{2}x}}
	\end{align*}
	with shrinkage operator
	\begin{align*}
		S_\kappa(x) = \brk*{\sgn(x_i)\brk*{\abs{x_i}-\kappa}_+}_i
	\end{align*}
	and some $\alpha\in\R$.
\end{frame}


\begin{frame}
	\begin{figure}
		\centering
		{\scriptsize
		\input{../Plots/method_comparison_ISTA.pgf}
		}
		\caption{Residual norms for the elastic net regression problem.}
	\end{figure}
\end{frame}

\begin{frame}
	\begin{figure}
		\centering
		{\scriptsize
		% \tiny
		\input{../Plots/memory_comparison_ISTA.pgf}
		}
		\caption{Residual norms for the elastic net regression problem.}
	\end{figure}
\end{frame}
\subsection{Elastic net regression}

\begin{frame}
	\frametitle{Markov decision process}
	Our aim is to find a fixed point of the Bellman operator
	\begin{align*}
		f\colon\R^{1000}\to\R^{1000}, \quad x\mapsto \brk*{\max_{a}R(s,a)+\gamma \sum_{s'}P(s,a,s')x_{s'}}_{s}
	\end{align*}
	with some $R\in\R^{300\times 200}$, $P\in\R^{300\times 200\times 300}$, $\gamma\in\R$.
\end{frame}


\begin{frame}
	\begin{figure}
		\centering
		{\scriptsize
		\input{../Plots/method_comparison_VI.pgf}
		}
		\caption{Residual norms for the elastic net regression problem.}
	\end{figure}
\end{frame}

\begin{frame}
	\begin{figure}
		\centering
		{\scriptsize
		% \tiny
		\input{../Plots/memory_comparison_VI.pgf}
		}
		\caption{Residual norms for the elastic net regression problem.}
	\end{figure}
\end{frame}

\section{Summary}

\begin{frame}
	\frametitle{Summary}
	\begin{itemize}
		\item aim is to find a fixed point of $f$ where
		\begin{itemize}
			\item the dimension is large
			\item $f$ is expensive to evaluate, noisy and the gradient is a mystery
		\end{itemize}
		\item 3 modifications to the AA-I algorithm yield well-definedness and convergence for non-expansive problems
		\begin{itemize}
			\item Powell-type regularisation
			\item Restarting iteration
			\item Safeguarding steps
		\end{itemize}
		\item 
	\end{itemize}
\end{frame}

\section{Sources}

\begin{frame}[allowframebreaks]
	\frametitle{Sources}
	\nocite{*}
%	\bibliographystyle{plain}
%	\bibliography{bibliography}
	\printbibliography
\end{frame}


\begin{frame}[plain]
	\begin{center}
		\Large{{Thank you for your attention.}}
	\end{center}
\end{frame}

\frame[plain]

\end{document}
